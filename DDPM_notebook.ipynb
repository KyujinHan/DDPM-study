{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abc43f74",
   "metadata": {},
   "source": [
    "# DDPM\n",
    "- Denoising Diffusion Probabilistic Models  \n",
    "\n",
    "Paper: https://arxiv.org/abs/2006.11239  \n",
    "Github: https://github.com/lucidrains/denoising-diffusion-pytorch  \n",
    "\n",
    "Image References: https://kyujinpy.tistory.com/95  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ce5550",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "```python\n",
    "from denoising_diffusion_pytorch import Unet, GaussianDiffusion, Trainer\n",
    "```\n",
    "코드를 보면 위의 module를 불러와서 DDPM을 구현합니다!   \n",
    "\n",
    "우리는 위의 모듈을 직접 코드로 전부 분석하면서 살펴봅시다!  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daabd9b8",
   "metadata": {},
   "source": [
    "# DDPM 공부할 때 참고한 자료\n",
    "- https://kyujinpy.tistory.com/95    \n",
    "- https://arxiv.org/pdf/2303.09556.pdf  \n",
    "- https://www.ai-bio.info/pytorch-register-buffer \n",
    "- https://m.blog.naver.com/chrhdhkd/222013835684 (FID)  \n",
    "- https://github.com/lucidrains/ema-pytorch (EMA github)\n",
    "- https://study-grow.tistory.com/entry/gema-EMA-%EA%B5%AC%ED%95%98%EB%8A%94-%EA%B3%B5%EC%8B%9D (EMA 공식)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeeab7a",
   "metadata": {},
   "source": [
    "# DDPM 코드에서 중요한 점 정리 (summary)\n",
    "- https://kyujinpy.tistory.com/123  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1806e83c",
   "metadata": {},
   "source": [
    "# DDPM implementation (detail)\n",
    "## Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d73ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unet class\n",
    "class Unet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        init_dim = None,\n",
    "        out_dim = None,\n",
    "        dim_mults = (1, 2, 4, 8),\n",
    "        channels = 3,\n",
    "        self_condition = False,\n",
    "        resnet_block_groups = 8,\n",
    "        learned_variance = False,\n",
    "        learned_sinusoidal_cond = False,\n",
    "        random_fourier_features = False,\n",
    "        learned_sinusoidal_dim = 16,\n",
    "        sinusoidal_pos_emb_theta = 10000,\n",
    "        attn_dim_head = 32,\n",
    "        attn_heads = 4,\n",
    "        full_attn = None,    # defaults to full attention only for inner most layer\n",
    "        flash_attn = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        #####################  determine dimensions\n",
    "        self.channels = channels\n",
    "        self.self_condition = self_condition\n",
    "        input_channels = channels * (2 if self_condition else 1)\n",
    "\n",
    "        init_dim = default(init_dim, dim) # init_dim과 dim 중에 선택\n",
    "        '''\n",
    "        def default(val, d):\n",
    "            if exists(val):\n",
    "                return val\n",
    "            return d() if callable(d) else d\n",
    "        '''\n",
    "        \n",
    "        self.init_conv = nn.Conv2d(input_channels, init_dim, 7, padding = 3)\n",
    "\n",
    "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "        \n",
    "        #####################  resnetBlock에 time embedding이 들어간다\n",
    "        block_klass = partial(ResnetBlock, groups = resnet_block_groups) # resnet Block 정의\n",
    "        '''\n",
    "        class ResnetBlock(nn.Module):\n",
    "            def __init__(self, dim, dim_out, *, time_emb_dim = None, groups = 8):\n",
    "                super().__init__()\n",
    "                self.mlp = nn.Sequential(\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(time_emb_dim, dim_out * 2)\n",
    "                ) if exists(time_emb_dim) else None\n",
    "\n",
    "                self.block1 = Block(dim, dim_out, groups = groups) # 아래에 정의\n",
    "                self.block2 = Block(dim_out, dim_out, groups = groups)\n",
    "                self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
    "\n",
    "            def forward(self, x, time_emb = None):\n",
    "                scale_shift = None\n",
    "                if exists(self.mlp) and exists(time_emb):\n",
    "                    time_emb = self.mlp(time_emb)\n",
    "                    time_emb = rearrange(time_emb, 'b c -> b c 1 1')\n",
    "                    scale_shift = time_emb.chunk(2, dim = 1)\n",
    "\n",
    "                h = self.block1(x, scale_shift = scale_shift)\n",
    "\n",
    "                h = self.block2(h)\n",
    "\n",
    "                return h + self.res_conv(x)\n",
    "                \n",
    "        class Block(nn.Module):\n",
    "            def __init__(self, dim, dim_out, groups = 8):\n",
    "                super().__init__()\n",
    "                self.proj = nn.Conv2d(dim, dim_out, 3, padding = 1)\n",
    "                self.norm = nn.GroupNorm(groups, dim_out)\n",
    "                self.act = nn.SiLU()\n",
    "\n",
    "            def forward(self, x, scale_shift = None):\n",
    "                x = self.proj(x)\n",
    "                x = self.norm(x)\n",
    "\n",
    "                if exists(scale_shift):\n",
    "                    scale, shift = scale_shift\n",
    "                    x = x * (scale + 1) + shift\n",
    "\n",
    "                x = self.act(x)\n",
    "                return x\n",
    "        '''\n",
    "        #################### \n",
    "\n",
    "        #################### time embeddings\n",
    "        time_dim = dim * 4\n",
    "\n",
    "        self.random_or_learned_sinusoidal_cond = learned_sinusoidal_cond or random_fourier_features # False\n",
    "        \n",
    "        # self.random_or_learned_sinusoidal_cond = False\n",
    "        # step t embedding\n",
    "        if self.random_or_learned_sinusoidal_cond:\n",
    "            sinu_pos_emb = RandomOrLearnedSinusoidalPosEmb(learned_sinusoidal_dim, random_fourier_features)\n",
    "            '''\n",
    "            class RandomOrLearnedSinusoidalPosEmb(nn.Module):\n",
    "                \"\"\" following @crowsonkb 's lead with random (learned optional) sinusoidal pos emb \"\"\"\n",
    "                \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n",
    "\n",
    "                def __init__(self, dim, is_random = False):\n",
    "                    super().__init__()\n",
    "                    assert divisible_by(dim, 2)\n",
    "                    half_dim = dim // 2\n",
    "                    self.weights = nn.Parameter(torch.randn(half_dim), requires_grad = not is_random)\n",
    "\n",
    "                def forward(self, x):\n",
    "                    x = rearrange(x, 'b -> b 1')\n",
    "                    freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n",
    "                    fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n",
    "                    fouriered = torch.cat((x, fouriered), dim = -1)\n",
    "                    return fouriered\n",
    "            '''\n",
    "            fourier_dim = learned_sinusoidal_dim + 1\n",
    "            \n",
    "        else:\n",
    "            sinu_pos_emb = SinusoidalPosEmb(dim, theta = sinusoidal_pos_emb_theta)\n",
    "            '''\n",
    "            class SinusoidalPosEmb(nn.Module):\n",
    "                def __init__(self, dim, theta = 10000):\n",
    "                    super().__init__()\n",
    "                    self.dim = dim\n",
    "                    self.theta = theta\n",
    "\n",
    "                def forward(self, x):\n",
    "                    device = x.device\n",
    "                    half_dim = self.dim // 2\n",
    "                    emb = math.log(self.theta) / (half_dim - 1)\n",
    "                    emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "                    emb = x[:, None] * emb[None, :]\n",
    "                    emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "                    return emb\n",
    "            '''\n",
    "            fourier_dim = dim\n",
    "        \n",
    "        ## time mlp? -> 일단 기억\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            sinu_pos_emb,\n",
    "            nn.Linear(fourier_dim, time_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_dim, time_dim)\n",
    "        ) \n",
    "        #################### \n",
    "        \n",
    "        ####################  attention 연산\n",
    "        if not full_attn:\n",
    "            full_attn = (*((False,) * (len(dim_mults) - 1)), True)\n",
    "\n",
    "        num_stages = len(dim_mults)\n",
    "        full_attn  = cast_tuple(full_attn, num_stages)\n",
    "        attn_heads = cast_tuple(attn_heads, num_stages)\n",
    "        attn_dim_head = cast_tuple(attn_dim_head, num_stages)\n",
    "\n",
    "        assert len(full_attn) == len(dim_mults)\n",
    "\n",
    "        FullAttention = partial(Attention, flash = flash_attn) # Attention 정의\n",
    "        '''\n",
    "        class Attention(nn.Module):\n",
    "            def __init__(\n",
    "                self,\n",
    "                dim,\n",
    "                heads = 4,\n",
    "                dim_head = 32,\n",
    "                num_mem_kv = 4,\n",
    "                flash = False\n",
    "            ):\n",
    "                super().__init__()\n",
    "                self.heads = heads\n",
    "                hidden_dim = dim_head * heads\n",
    "\n",
    "                self.norm = RMSNorm(dim)\n",
    "                self.attend = Attend(flash = flash) # 아래에 정의\n",
    "\n",
    "                self.mem_kv = nn.Parameter(torch.randn(2, heads, num_mem_kv, dim_head))\n",
    "                self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)\n",
    "                self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
    "\n",
    "            def forward(self, x):\n",
    "                b, c, h, w = x.shape\n",
    "\n",
    "                x = self.norm(x)\n",
    "\n",
    "                qkv = self.to_qkv(x).chunk(3, dim = 1) # output 3개\n",
    "                q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h (x y) c', h = self.heads), qkv)\n",
    "\n",
    "                mk, mv = map(lambda t: repeat(t, 'h n d -> b h n d', b = b), self.mem_kv) # random parameters\n",
    "                k, v = map(partial(torch.cat, dim = -2), ((mk, k), (mv, v))) \n",
    "\n",
    "                out = self.attend(q, k, v)\n",
    "\n",
    "                out = rearrange(out, 'b h (x y) d -> b (h d) x y', x = h, y = w)\n",
    "                return self.to_out(out)\n",
    "        '''\n",
    "        '''\n",
    "        class Attend(nn.Module):\n",
    "            def __init__(\n",
    "                self,\n",
    "                dropout = 0.,\n",
    "                flash = False\n",
    "            ):\n",
    "                super().__init__()\n",
    "                self.dropout = dropout\n",
    "                self.attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "                self.flash = flash\n",
    "                assert not (flash and version.parse(torch.__version__) < version.parse('2.0.0')), 'in order to use flash attention, you must be using pytorch 2.0 or above'\n",
    "\n",
    "                # determine efficient attention configs for cuda and cpu\n",
    "\n",
    "                self.cpu_config = AttentionConfig(True, True, True)\n",
    "                self.cuda_config = None\n",
    "\n",
    "                if not torch.cuda.is_available() or not flash:\n",
    "                    return\n",
    "\n",
    "                device_properties = torch.cuda.get_device_properties(torch.device('cuda'))\n",
    "\n",
    "                if device_properties.major == 8 and device_properties.minor == 0:\n",
    "                    print_once('A100 GPU detected, using flash attention if input tensor is on cuda')\n",
    "                    self.cuda_config = AttentionConfig(True, False, False)\n",
    "                else:\n",
    "                    print_once('Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda')\n",
    "                    self.cuda_config = AttentionConfig(False, True, True)\n",
    "\n",
    "            def flash_attn(self, q, k, v):\n",
    "                _, heads, q_len, _, k_len, is_cuda, device = *q.shape, k.shape[-2], q.is_cuda, q.device\n",
    "\n",
    "                q, k, v = map(lambda t: t.contiguous(), (q, k, v))\n",
    "\n",
    "                # Check if there is a compatible device for flash attention\n",
    "\n",
    "                config = self.cuda_config if is_cuda else self.cpu_config\n",
    "\n",
    "                # pytorch 2.0 flash attn: q, k, v, mask, dropout, causal, softmax_scale\n",
    "\n",
    "                with torch.backends.cuda.sdp_kernel(**config._asdict()):\n",
    "                    out = F.scaled_dot_product_attention(\n",
    "                        q, k, v,\n",
    "                        dropout_p = self.dropout if self.training else 0.\n",
    "                    )\n",
    "\n",
    "                return out\n",
    "\n",
    "            def forward(self, q, k, v):\n",
    "                \"\"\"\n",
    "                einstein notation\n",
    "                b - batch\n",
    "                h - heads\n",
    "                n, i, j - sequence length (base sequence length, source, target)\n",
    "                d - feature dimension\n",
    "                \"\"\"\n",
    "\n",
    "                q_len, k_len, device = q.shape[-2], k.shape[-2], q.device\n",
    "\n",
    "                if self.flash:\n",
    "                    return self.flash_attn(q, k, v)\n",
    "\n",
    "                scale = q.shape[-1] ** -0.5\n",
    "\n",
    "                # similarity\n",
    "\n",
    "                sim = einsum(f\"b h i d, b h j d -> b h i j\", q, k) * scale\n",
    "\n",
    "                # attention\n",
    "\n",
    "                attn = sim.softmax(dim = -1)\n",
    "                attn = self.attn_dropout(attn)\n",
    "\n",
    "                # aggregate values\n",
    "\n",
    "                out = einsum(f\"b h i j, b h j d -> b h i d\", attn, v)\n",
    "\n",
    "                return out\n",
    "        '''\n",
    "        #################### \n",
    "\n",
    "        # layers\n",
    "        self.downs = nn.ModuleList([])\n",
    "        self.ups = nn.ModuleList([])\n",
    "        num_resolutions = len(in_out)\n",
    "\n",
    "        for ind, ((dim_in, dim_out), layer_full_attn, layer_attn_heads, layer_attn_dim_head) in enumerate(zip(in_out, full_attn, attn_heads, attn_dim_head)):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "\n",
    "            attn_klass = FullAttention if layer_full_attn else LinearAttention\n",
    "\n",
    "            self.downs.append(nn.ModuleList([\n",
    "                block_klass(dim_in, dim_in, time_emb_dim = time_dim),\n",
    "                block_klass(dim_in, dim_in, time_emb_dim = time_dim),\n",
    "                attn_klass(dim_in, dim_head = layer_attn_dim_head, heads = layer_attn_heads),\n",
    "                Downsample(dim_in, dim_out) if not is_last else nn.Conv2d(dim_in, dim_out, 3, padding = 1)\n",
    "            ]))\n",
    "\n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim = time_dim)\n",
    "        self.mid_attn = FullAttention(mid_dim, heads = attn_heads[-1], dim_head = attn_dim_head[-1])\n",
    "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim = time_dim)\n",
    "\n",
    "        for ind, ((dim_in, dim_out), layer_full_attn, layer_attn_heads, layer_attn_dim_head) in enumerate(zip(*map(reversed, (in_out, full_attn, attn_heads, attn_dim_head)))):\n",
    "            is_last = ind == (len(in_out) - 1)\n",
    "\n",
    "            attn_klass = FullAttention if layer_full_attn else LinearAttention\n",
    "\n",
    "            self.ups.append(nn.ModuleList([\n",
    "                block_klass(dim_out + dim_in, dim_out, time_emb_dim = time_dim),\n",
    "                block_klass(dim_out + dim_in, dim_out, time_emb_dim = time_dim),\n",
    "                attn_klass(dim_out, dim_head = layer_attn_dim_head, heads = layer_attn_heads),\n",
    "                Upsample(dim_out, dim_in) if not is_last else  nn.Conv2d(dim_out, dim_in, 3, padding = 1)\n",
    "            ]))\n",
    "\n",
    "        default_out_dim = channels * (1 if not learned_variance else 2)\n",
    "        self.out_dim = default(out_dim, default_out_dim)\n",
    "\n",
    "        self.final_res_block = block_klass(dim * 2, dim, time_emb_dim = time_dim)\n",
    "        self.final_conv = nn.Conv2d(dim, self.out_dim, 1)\n",
    "\n",
    "    @property\n",
    "    def downsample_factor(self):\n",
    "        return 2 ** (len(self.downs) - 1)\n",
    "\n",
    "    def forward(self, x, time, x_self_cond = None):\n",
    "        assert all([divisible_by(d, self.downsample_factor) for d in x.shape[-2:]]), f'your input dimensions {x.shape[-2:]} need to be divisible by {self.downsample_factor}, given the unet'\n",
    "\n",
    "        if self.self_condition: # False\n",
    "            x_self_cond = default(x_self_cond, lambda: torch.zeros_like(x))\n",
    "            x = torch.cat((x_self_cond, x), dim = 1)\n",
    "\n",
    "        x = self.init_conv(x)\n",
    "        r = x.clone()\n",
    "\n",
    "        t = self.time_mlp(time) # t step에 관련된 t-value\n",
    "\n",
    "        h = []\n",
    "\n",
    "        for block1, block2, attn, downsample in self.downs:\n",
    "            x = block1(x, t)\n",
    "            h.append(x)\n",
    "\n",
    "            x = block2(x, t)\n",
    "            x = attn(x) + x\n",
    "            h.append(x)\n",
    "\n",
    "            x = downsample(x)\n",
    "\n",
    "        x = self.mid_block1(x, t)\n",
    "        x = self.mid_attn(x) + x\n",
    "        x = self.mid_block2(x, t)\n",
    "\n",
    "        for block1, block2, attn, upsample in self.ups:\n",
    "            x = torch.cat((x, h.pop()), dim = 1) # skip connection\n",
    "            x = block1(x, t)\n",
    "\n",
    "            x = torch.cat((x, h.pop()), dim = 1)\n",
    "            x = block2(x, t)\n",
    "            x = attn(x) + x\n",
    "\n",
    "            x = upsample(x)\n",
    "\n",
    "        x = torch.cat((x, r), dim = 1) # r = x = self.init_conv(x)\n",
    "\n",
    "        x = self.final_res_block(x, t)\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c0852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 활용\n",
    "model = Unet(\n",
    "    dim = 64,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    flash_attn = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c41584e",
   "metadata": {},
   "source": [
    "## GaussianDiffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f118815",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDiffusion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        *,\n",
    "        image_size,\n",
    "        timesteps = 1000,\n",
    "        sampling_timesteps = None,\n",
    "        objective = 'pred_v',\n",
    "        beta_schedule = 'sigmoid',\n",
    "        schedule_fn_kwargs = dict(),\n",
    "        ddim_sampling_eta = 0.,\n",
    "        auto_normalize = True,\n",
    "        offset_noise_strength = 0.,  # https://www.crosslabs.org/blog/diffusion-with-offset-noise\n",
    "        min_snr_loss_weight = False, # https://arxiv.org/abs/2303.09556\n",
    "        min_snr_gamma = 5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert not (type(self) == GaussianDiffusion and model.channels != model.out_dim)\n",
    "        assert not model.random_or_learned_sinusoidal_cond\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "        self.channels = self.model.channels\n",
    "        self.self_condition = self.model.self_condition\n",
    "\n",
    "        self.image_size = image_size\n",
    "\n",
    "        self.objective = objective\n",
    "\n",
    "        assert objective in {'pred_noise', 'pred_x0', 'pred_v'}, 'objective must be either pred_noise (predict noise) or pred_x0 (predict image start) or pred_v (predict v [v-parameterization as defined in appendix D of progressive distillation paper, used in imagen-video successfully])'\n",
    "        \n",
    "        ######################## beta scheduler-> sigmoid\n",
    "        ## DDPM은 학습에 이용되는 beta_t의 값을 constant하게 유지하기 때문에\n",
    "        if beta_schedule == 'linear':\n",
    "            beta_schedule_fn = linear_beta_schedule\n",
    "        elif beta_schedule == 'cosine':\n",
    "            beta_schedule_fn = cosine_beta_schedule\n",
    "        elif beta_schedule == 'sigmoid':\n",
    "            beta_schedule_fn = sigmoid_beta_schedule\n",
    "            '''\n",
    "            def sigmoid_beta_schedule(timesteps, start = -3, end = 3, tau = 1, clamp_min = 1e-5):\n",
    "                \"\"\"\n",
    "                sigmoid schedule\n",
    "                proposed in https://arxiv.org/abs/2212.11972 - Figure 8\n",
    "                better for images > 64x64, when used during training\n",
    "                \"\"\"\n",
    "                steps = timesteps + 1\n",
    "                t = torch.linspace(0, timesteps, steps, dtype = torch.float64) / timesteps\n",
    "                v_start = torch.tensor(start / tau).sigmoid()\n",
    "                v_end = torch.tensor(end / tau).sigmoid()\n",
    "                alphas_cumprod = (-((t * (end - start) + start) / tau).sigmoid() + v_end) / (v_end - v_start)\n",
    "                alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "                betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "                return torch.clip(betas, 0, 0.999)\n",
    "            '''\n",
    "        else:\n",
    "            raise ValueError(f'unknown beta schedule {beta_schedule}')\n",
    "        \n",
    "        # timesteps = 1000\n",
    "        # -> steos = 1001\n",
    "        betas = beta_schedule_fn(timesteps, **schedule_fn_kwargs)\n",
    "        ########################\n",
    "        \n",
    "        ######################### 논문 정의\n",
    "        ## alpha를 통하여서, 빠른 전개가 가능하도록 함.\n",
    "        alphas = 1. - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0) # t개만큼의 누적곱\n",
    "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value = 1.)\n",
    "        \n",
    "        ########################\n",
    "        \n",
    "        ######################## time t\n",
    "        timesteps, = betas.shape # betas의 shape ==> time stamp 개수\n",
    "        self.num_timesteps = int(timesteps)\n",
    "\n",
    "        # sampling related parameters\n",
    "        ## sampling_timesteps = 250\n",
    "        self.sampling_timesteps = default(sampling_timesteps, timesteps) # default num sampling timesteps to number of timesteps at training\n",
    "        ########################\n",
    "        \n",
    "        ######################## 논문에서 이용되는 수식 정의\n",
    "        assert self.sampling_timesteps <= timesteps\n",
    "        self.is_ddim_sampling = self.sampling_timesteps < timesteps # ddim 목록 (나중에)\n",
    "        self.ddim_sampling_eta = ddim_sampling_eta # ddim 목록 (나중에)\n",
    "\n",
    "        # helper function to register buffer from float64 to float32\n",
    "        ## register_buffer를 써야하는 이유: https://www.ai-bio.info/pytorch-register-buffer\n",
    "        ### register_buffer에 등록된 애들은 자동으로 cuda로 옮기기 편하다.\n",
    "        #### register_buffer에 등록된 애들은 학습 되지 않는다! (아주 중요!!)\n",
    "        register_buffer = lambda name, val: self.register_buffer(name, val.to(torch.float32))\n",
    "\n",
    "        register_buffer('betas', betas)\n",
    "        register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n",
    "\n",
    "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "\n",
    "        register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod)) # forward process의 q(Xt|X0)\n",
    "        register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - alphas_cumprod)) # forward process의 q(Xt|X0)\n",
    "        register_buffer('log_one_minus_alphas_cumprod', torch.log(1. - alphas_cumprod))\n",
    "        register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1. / alphas_cumprod))\n",
    "        register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1. / alphas_cumprod - 1))\n",
    "\n",
    "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "        ##  논문에서는 간단하게 beta_t를 분산으로 쓰는 것도 제안함\n",
    "        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod) \n",
    "\n",
    "        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n",
    "\n",
    "        register_buffer('posterior_variance', posterior_variance)\n",
    "\n",
    "        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n",
    "\n",
    "        register_buffer('posterior_log_variance_clipped', torch.log(posterior_variance.clamp(min =1e-20)))\n",
    "        register_buffer('posterior_mean_coef1', betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod))\n",
    "        register_buffer('posterior_mean_coef2', (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod))\n",
    "        #########################\n",
    "        \n",
    "        ######################### offset noise strength - in blogpost, they claimed 0.1 was ideal\n",
    "        self.offset_noise_strength = offset_noise_strength\n",
    "        #########################\n",
    "        \n",
    "        ########################## derive loss weight\n",
    "        # snr - signal noise ratio\n",
    "        ## Check: 언제 사용될까? 좀 지켜보기\n",
    "        ### -> loss * self.loss_weight \n",
    "        \n",
    "        snr = alphas_cumprod / (1 - alphas_cumprod)\n",
    "        \n",
    "        # https://arxiv.org/abs/2303.09556\n",
    "\n",
    "        maybe_clipped_snr = snr.clone()\n",
    "        if min_snr_loss_weight:\n",
    "            maybe_clipped_snr.clamp_(max = min_snr_gamma) # min_snr_gamma\n",
    "        \n",
    "        # default: pred_v\n",
    "        if objective == 'pred_noise': # 기존의 DDPM: noise 예측\n",
    "            register_buffer('loss_weight', maybe_clipped_snr / snr)\n",
    "        elif objective == 'pred_x0': # x0를 예측하는 것\n",
    "            register_buffer('loss_weight', maybe_clipped_snr)\n",
    "            \n",
    "        ## velocity v\n",
    "        #### https://arxiv.org/abs/2112.10752\n",
    "        ##### Check: 더 살펴보기\n",
    "        elif objective == 'pred_v':\n",
    "            register_buffer('loss_weight', maybe_clipped_snr / (snr + 1))\n",
    "\n",
    "        ########################### auto-normalization of data [0, 1] -> [-1, 1] - can turn off by setting it to be False\n",
    "        self.normalize = normalize_to_neg_one_to_one if auto_normalize else identity\n",
    "        self.unnormalize = unnormalize_to_zero_to_one if auto_normalize else identity\n",
    "        ##########################\n",
    "        \n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.betas.device\n",
    "\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        return (\n",
    "            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n",
    "            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n",
    "        )\n",
    "\n",
    "    def predict_noise_from_start(self, x_t, t, x0):\n",
    "        return (\n",
    "            (extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - x0) / \\\n",
    "            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n",
    "        )\n",
    "\n",
    "    def predict_v(self, x_start, t, noise):\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * noise -\n",
    "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * x_start\n",
    "        )\n",
    "\n",
    "    def predict_start_from_v(self, x_t, t, v):\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_t.shape) * x_t -\n",
    "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_t.shape) * v\n",
    "        )\n",
    "\n",
    "    def q_posterior(self, x_start, x_t, t):\n",
    "        posterior_mean = (\n",
    "            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n",
    "            extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        )\n",
    "        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n",
    "        posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "\n",
    "    def model_predictions(self, x, t, x_self_cond = None, clip_x_start = False, rederive_pred_noise = False):\n",
    "        model_output = self.model(x, t, x_self_cond)\n",
    "        maybe_clip = partial(torch.clamp, min = -1., max = 1.) if clip_x_start else identity\n",
    "\n",
    "        if self.objective == 'pred_noise':\n",
    "            pred_noise = model_output\n",
    "            x_start = self.predict_start_from_noise(x, t, pred_noise)\n",
    "            x_start = maybe_clip(x_start)\n",
    "\n",
    "            if clip_x_start and rederive_pred_noise:\n",
    "                pred_noise = self.predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "        elif self.objective == 'pred_x0':\n",
    "            x_start = model_output\n",
    "            x_start = maybe_clip(x_start)\n",
    "            pred_noise = self.predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "        elif self.objective == 'pred_v':\n",
    "            v = model_output\n",
    "            x_start = self.predict_start_from_v(x, t, v)\n",
    "            x_start = maybe_clip(x_start)\n",
    "            pred_noise = self.predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "        return ModelPrediction(pred_noise, x_start)\n",
    "\n",
    "    def p_mean_variance(self, x, t, x_self_cond = None, clip_denoised = True):\n",
    "        preds = self.model_predictions(x, t, x_self_cond)\n",
    "        x_start = preds.pred_x_start\n",
    "\n",
    "        if clip_denoised:\n",
    "            x_start.clamp_(-1., 1.)\n",
    "\n",
    "        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start = x_start, x_t = x, t = t)\n",
    "        return model_mean, posterior_variance, posterior_log_variance, x_start\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def p_sample(self, x, t: int, x_self_cond = None):\n",
    "        b, *_, device = *x.shape, self.device\n",
    "        batched_times = torch.full((b,), t, device = device, dtype = torch.long)\n",
    "        model_mean, _, model_log_variance, x_start = self.p_mean_variance(x = x, t = batched_times, x_self_cond = x_self_cond, clip_denoised = True)\n",
    "        noise = torch.randn_like(x) if t > 0 else 0. # no noise if t == 0\n",
    "        pred_img = model_mean + (0.5 * model_log_variance).exp() * noise\n",
    "        return pred_img, x_start\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def p_sample_loop(self, shape, return_all_timesteps = False):\n",
    "        batch, device = shape[0], self.device\n",
    "\n",
    "        img = torch.randn(shape, device = device)\n",
    "        imgs = [img]\n",
    "\n",
    "        x_start = None\n",
    "\n",
    "        for t in tqdm(reversed(range(0, self.num_timesteps)), desc = 'sampling loop time step', total = self.num_timesteps):\n",
    "            self_cond = x_start if self.self_condition else None\n",
    "            img, x_start = self.p_sample(img, t, self_cond)\n",
    "            imgs.append(img)\n",
    "\n",
    "        ret = img if not return_all_timesteps else torch.stack(imgs, dim = 1)\n",
    "\n",
    "        ret = self.unnormalize(ret)\n",
    "        return ret\n",
    "    \n",
    "    ########  나중에~ 살펴보기\n",
    "    @torch.inference_mode()\n",
    "    # ddim\n",
    "    def ddim_sample(self, shape, return_all_timesteps = False):\n",
    "        batch, device, total_timesteps, sampling_timesteps, eta, objective = shape[0], self.device, self.num_timesteps, self.sampling_timesteps, self.ddim_sampling_eta, self.objective\n",
    "\n",
    "        times = torch.linspace(-1, total_timesteps - 1, steps = sampling_timesteps + 1)   # [-1, 0, 1, 2, ..., T-1] when sampling_timesteps == total_timesteps\n",
    "        times = list(reversed(times.int().tolist()))\n",
    "        time_pairs = list(zip(times[:-1], times[1:])) # [(T-1, T-2), (T-2, T-3), ..., (1, 0), (0, -1)]\n",
    "\n",
    "        img = torch.randn(shape, device = device)\n",
    "        imgs = [img]\n",
    "\n",
    "        x_start = None\n",
    "\n",
    "        for time, time_next in tqdm(time_pairs, desc = 'sampling loop time step'):\n",
    "            time_cond = torch.full((batch,), time, device = device, dtype = torch.long)\n",
    "            self_cond = x_start if self.self_condition else None\n",
    "            pred_noise, x_start, *_ = self.model_predictions(img, time_cond, self_cond, clip_x_start = True, rederive_pred_noise = True)\n",
    "\n",
    "            if time_next < 0:\n",
    "                img = x_start\n",
    "                imgs.append(img)\n",
    "                continue\n",
    "\n",
    "            alpha = self.alphas_cumprod[time]\n",
    "            alpha_next = self.alphas_cumprod[time_next]\n",
    "\n",
    "            sigma = eta * ((1 - alpha / alpha_next) * (1 - alpha_next) / (1 - alpha)).sqrt()\n",
    "            c = (1 - alpha_next - sigma ** 2).sqrt()\n",
    "\n",
    "            noise = torch.randn_like(img)\n",
    "\n",
    "            img = x_start * alpha_next.sqrt() + \\\n",
    "                  c * pred_noise + \\\n",
    "                  sigma * noise\n",
    "\n",
    "            imgs.append(img)\n",
    "\n",
    "        ret = img if not return_all_timesteps else torch.stack(imgs, dim = 1)\n",
    "\n",
    "        ret = self.unnormalize(ret)\n",
    "        return ret\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def sample(self, batch_size = 16, return_all_timesteps = False):\n",
    "        image_size, channels = self.image_size, self.channels\n",
    "        sample_fn = self.p_sample_loop if not self.is_ddim_sampling else self.ddim_sample\n",
    "        return sample_fn((batch_size, channels, image_size, image_size), return_all_timesteps = return_all_timesteps)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def interpolate(self, x1, x2, t = None, lam = 0.5):\n",
    "        b, *_, device = *x1.shape, x1.device\n",
    "        t = default(t, self.num_timesteps - 1)\n",
    "\n",
    "        assert x1.shape == x2.shape\n",
    "\n",
    "        t_batched = torch.full((b,), t, device = device)\n",
    "        xt1, xt2 = map(lambda x: self.q_sample(x, t = t_batched), (x1, x2))\n",
    "\n",
    "        img = (1 - lam) * xt1 + lam * xt2\n",
    "\n",
    "        x_start = None\n",
    "\n",
    "        for i in tqdm(reversed(range(0, t)), desc = 'interpolation sample time step', total = t):\n",
    "            self_cond = x_start if self.self_condition else None\n",
    "            img, x_start = self.p_sample(img, i, self_cond)\n",
    "\n",
    "        return img\n",
    "\n",
    "    @autocast(enabled = False)\n",
    "    def q_sample(self, x_start, t, noise = None):\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start + # alphas_cumprod = torch.cumprod(alphas, dim=0) # t개만큼의 누적곱\n",
    "            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise # torch.sqrt(1. - alphas_cumprod)\n",
    "        ) ## (평균*x0 + 표준편차*eta) # eta ~ N(0,1) ==> Diffusino 논문에서 소개하는 q(Xt | X0) 수식\n",
    "    '''\n",
    "    def extract(a, t, x_shape):\n",
    "        b, *_ = t.shape # batch size\n",
    "        out = a.gather(-1, t) # 여러 alpha 값 중에서 \n",
    "        return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "    '''\n",
    "    \n",
    "    # reverse process\n",
    "    def p_losses(self, x_start, t, noise = None, offset_noise_strength = None):\n",
    "        b, c, h, w = x_start.shape\n",
    "\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start)) # randn_like -> 정규분포 따름 (diffusion process이므로 당연!)\n",
    "        \n",
    "        ###### offset noise - https://www.crosslabs.org/blog/diffusion-with-offset-noise\n",
    "        offset_noise_strength = default(offset_noise_strength, self.offset_noise_strength)\n",
    "        if offset_noise_strength > 0.:\n",
    "            offset_noise = torch.randn(x_start.shape[:2], device = self.device)\n",
    "            noise += offset_noise_strength * rearrange(offset_noise, 'b c -> b c 1 1')\n",
    "        ######\n",
    "        \n",
    "        ####### noise sample\n",
    "        x = self.q_sample(x_start = x_start, t = t, noise = noise) # diffusion process\n",
    "        '''\n",
    "        def q_sample(self, x_start, t, noise = None):\n",
    "            noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "\n",
    "            return (\n",
    "                extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n",
    "                extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n",
    "            )\n",
    "        '''\n",
    "        #######\n",
    "        \n",
    "        #######\n",
    "        # if doing self-conditioning, 50% of the time, predict x_start from current set of times\n",
    "        # and condition with unet with that\n",
    "        # this technique will slow down training by 25%, but seems to lower FID significantly\n",
    "\n",
    "        x_self_cond = None\n",
    "        if self.self_condition and random() < 0.5: ### self_condition 나중에 살펴보기 // unet에서 정의할 때는 False\n",
    "            with torch.inference_mode():\n",
    "                x_self_cond = self.model_predictions(x, t).pred_x_start\n",
    "                x_self_cond.detach_()\n",
    "        #######\n",
    "\n",
    "        ######## predict and take gradient step\n",
    "        ## model에 넣기 -> unet + attention\n",
    "        model_out = self.model(x, t, x_self_cond)\n",
    "\n",
    "        if self.objective == 'pred_noise':\n",
    "            target = noise\n",
    "        elif self.objective == 'pred_x0':\n",
    "            target = x_start\n",
    "        elif self.objective == 'pred_v': #################### what is it?\n",
    "            v = self.predict_v(x_start, t, noise)\n",
    "            target = v\n",
    "            '''\n",
    "            def predict_v(self, x_start, t, noise):\n",
    "                return (\n",
    "                    extract(self.sqrt_alphas_cumprod, t, x_start.shape) * noise -\n",
    "                    extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * x_start\n",
    "                )\n",
    "            '''\n",
    "        else:\n",
    "            raise ValueError(f'unknown objective {self.objective}')\n",
    "        #######\n",
    "        \n",
    "        ####### loss 계산\n",
    "        loss = F.mse_loss(model_out, target, reduction = 'none')\n",
    "        loss = reduce(loss, 'b ... -> b', 'mean') \n",
    "\n",
    "        loss = loss * extract(self.loss_weight, t, loss.shape) ## self.loss_weight -> 각 t번째 마다 각각 다른 weight를 할당함.\n",
    "        return loss.mean # batch의 평균\n",
    "        #######\n",
    "    \n",
    "    # forward!!\n",
    "    def forward(self, img, *args, **kwargs):\n",
    "        b, c, h, w, device, img_size, = *img.shape, img.device, self.image_size\n",
    "        assert h == img_size and w == img_size, f'height and width of image must be {img_size}'\n",
    "        \n",
    "        \n",
    "        t = torch.randint(0, self.num_timesteps, (b,), device=device).long() # num_timesteps: 1000 // b: size (batch size)\n",
    "        # t를 무작정 1~1000까지 다 생성하는게 아니고,\n",
    "        ### 이미지 하나당 무작위 수의 t를 하나씩 매핑해서, 그 t에 맞는 random noise를 생성한 후, loss 계산 (중요!!)\n",
    "\n",
    "        img = self.normalize(img) # norm\n",
    "        return self.p_losses(img, t, *args, **kwargs) # p_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01726e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 활용\n",
    "diffusion = GaussianDiffusion(\n",
    "    model,\n",
    "    image_size = 128,\n",
    "    timesteps = 1000,           # number of steps\n",
    "    sampling_timesteps = 250    # number of sampling timesteps (using ddim for faster inference [see citation for ddim paper])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77076e5c",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a9f1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        diffusion_model,\n",
    "        folder,\n",
    "        *,\n",
    "        train_batch_size = 16,\n",
    "        gradient_accumulate_every = 1,\n",
    "        augment_horizontal_flip = True,\n",
    "        train_lr = 1e-4,\n",
    "        train_num_steps = 100000,\n",
    "        ema_update_every = 10,\n",
    "        ema_decay = 0.995,\n",
    "        adam_betas = (0.9, 0.99),\n",
    "        save_and_sample_every = 1000,\n",
    "        num_samples = 25,\n",
    "        results_folder = './results',\n",
    "        amp = False,\n",
    "        mixed_precision_type = 'fp16',\n",
    "        split_batches = True,\n",
    "        convert_image_to = None,\n",
    "        calculate_fid = True,\n",
    "        inception_block_idx = 2048,\n",
    "        max_grad_norm = 1.,\n",
    "        num_fid_samples = 50000,\n",
    "        save_best_and_latest_only = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # accelerator\n",
    "\n",
    "        self.accelerator = Accelerator(\n",
    "            split_batches = split_batches,\n",
    "            mixed_precision = mixed_precision_type if amp else 'no'\n",
    "        )\n",
    "\n",
    "        ############### model\n",
    "        self.model = diffusion_model # GaussianDiffusion\n",
    "        self.channels = diffusion_model.channels\n",
    "        is_ddim_sampling = diffusion_model.is_ddim_sampling\n",
    "\n",
    "        # default convert_image_to depending on channels\n",
    "\n",
    "        if not exists(convert_image_to):\n",
    "            convert_image_to = {1: 'L', 3: 'RGB', 4: 'RGBA'}.get(self.channels)\n",
    "\n",
    "        # sampling and training hyperparameters\n",
    "\n",
    "        assert has_int_squareroot(num_samples), 'number of samples must have an integer square root'\n",
    "        self.num_samples = num_samples\n",
    "        self.save_and_sample_every = save_and_sample_every\n",
    "\n",
    "        self.batch_size = train_batch_size\n",
    "        self.gradient_accumulate_every = gradient_accumulate_every\n",
    "        assert (train_batch_size * gradient_accumulate_every) >= 16, f'your effective batch size (train_batch_size x gradient_accumulate_every) should be at least 16 or above'\n",
    "\n",
    "        self.train_num_steps = train_num_steps\n",
    "        self.image_size = diffusion_model.image_size\n",
    "\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        ###############\n",
    "\n",
    "        ################ dataset and dataloader\n",
    "        self.ds = Dataset(folder, self.image_size, augment_horizontal_flip = augment_horizontal_flip, convert_image_to = convert_image_to)\n",
    "\n",
    "        assert len(self.ds) >= 100, 'you should have at least 100 images in your folder. at least 10k images recommended'\n",
    "\n",
    "        dl = DataLoader(self.ds, batch_size = train_batch_size, shuffle = True, pin_memory = True, num_workers = cpu_count())\n",
    "\n",
    "        dl = self.accelerator.prepare(dl)\n",
    "        self.dl = cycle(dl)\n",
    "        ###############\n",
    "        \n",
    "        ################ optimizer\n",
    "        self.opt = Adam(diffusion_model.parameters(), lr = train_lr, betas = adam_betas)\n",
    "\n",
    "        # for logging results in a folder periodically\n",
    "        if self.accelerator.is_main_process:\n",
    "            # from ema_pytorch import EMA\n",
    "            ## https://github.com/lucidrains/ema-pytorch\n",
    "            ## https://study-grow.tistory.com/entry/gema-EMA-%EA%B5%AC%ED%95%98%EB%8A%94-%EA%B3%B5%EC%8B%9D\n",
    "            ### EMA: exponential moving average\n",
    "            #### -> EMA로 계산한 가중치를 model에 넣어서 이용 (마지막에)\n",
    "            self.ema = EMA(diffusion_model, beta = ema_decay, update_every = ema_update_every) \n",
    "            self.ema.to(self.device)\n",
    "\n",
    "        self.results_folder = Path(results_folder)\n",
    "        self.results_folder.mkdir(exist_ok = True)\n",
    "\n",
    "        # step counter state\n",
    "\n",
    "        self.step = 0\n",
    "\n",
    "        # prepare model, dataloader, optimizer with accelerator\n",
    "\n",
    "        self.model, self.opt = self.accelerator.prepare(self.model, self.opt)\n",
    "        ###############\n",
    "\n",
    "        ################ FID-score computation\n",
    "        # FID는 생성된 영상의 집합과 실제 생성하고자 하는 클래스 데이터의 분포의 거리를 계산한다. \n",
    "        ## 거리가 가까울수록 좋은 영상으로 판단한다.\n",
    "        self.calculate_fid = calculate_fid and self.accelerator.is_main_process\n",
    "\n",
    "        if self.calculate_fid:\n",
    "            if not is_ddim_sampling:\n",
    "                self.accelerator.print(\n",
    "                    \"WARNING: Robust FID computation requires a lot of generated samples and can therefore be very time consuming.\"\\\n",
    "                    \"Consider using DDIM sampling to save time.\"\n",
    "                )\n",
    "            self.fid_scorer = FIDEvaluation(\n",
    "                batch_size=self.batch_size,\n",
    "                dl=self.dl,\n",
    "                sampler=self.ema.ema_model,\n",
    "                channels=self.channels,\n",
    "                accelerator=self.accelerator,\n",
    "                stats_dir=results_folder,\n",
    "                device=self.device,\n",
    "                num_fid_samples=num_fid_samples,\n",
    "                inception_block_idx=inception_block_idx\n",
    "            )\n",
    "        ################\n",
    "\n",
    "        if save_best_and_latest_only:\n",
    "            assert calculate_fid, \"`calculate_fid` must be True to provide a means for model evaluation for `save_best_and_latest_only`.\"\n",
    "            self.best_fid = 1e10 # infinite\n",
    "\n",
    "        self.save_best_and_latest_only = save_best_and_latest_only\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.accelerator.device\n",
    "\n",
    "    def save(self, milestone):\n",
    "        if not self.accelerator.is_local_main_process:\n",
    "            return\n",
    "\n",
    "        data = {\n",
    "            'step': self.step,\n",
    "            'model': self.accelerator.get_state_dict(self.model),\n",
    "            'opt': self.opt.state_dict(),\n",
    "            'ema': self.ema.state_dict(),\n",
    "            'scaler': self.accelerator.scaler.state_dict() if exists(self.accelerator.scaler) else None,\n",
    "            'version': __version__\n",
    "        }\n",
    "\n",
    "        torch.save(data, str(self.results_folder / f'model-{milestone}.pt'))\n",
    "\n",
    "    def load(self, milestone):\n",
    "        accelerator = self.accelerator\n",
    "        device = accelerator.device\n",
    "\n",
    "        data = torch.load(str(self.results_folder / f'model-{milestone}.pt'), map_location=device)\n",
    "\n",
    "        model = self.accelerator.unwrap_model(self.model)\n",
    "        model.load_state_dict(data['model'])\n",
    "\n",
    "        self.step = data['step']\n",
    "        self.opt.load_state_dict(data['opt'])\n",
    "        if self.accelerator.is_main_process:\n",
    "            self.ema.load_state_dict(data[\"ema\"])\n",
    "\n",
    "        if 'version' in data:\n",
    "            print(f\"loading from version {data['version']}\")\n",
    "\n",
    "        if exists(self.accelerator.scaler) and exists(data['scaler']):\n",
    "            self.accelerator.scaler.load_state_dict(data['scaler'])\n",
    "    \n",
    "    ################# 훈련!!\n",
    "    def train(self):\n",
    "        accelerator = self.accelerator\n",
    "        device = accelerator.device\n",
    "\n",
    "        with tqdm(initial = self.step, total = self.train_num_steps, disable = not accelerator.is_main_process) as pbar:\n",
    "\n",
    "            while self.step < self.train_num_steps: # 700000\n",
    "\n",
    "                total_loss = 0.\n",
    "\n",
    "                for _ in range(self.gradient_accumulate_every):\n",
    "                    data = next(self.dl).to(device)\n",
    "\n",
    "                    with self.accelerator.autocast():\n",
    "                        loss = self.model(data)\n",
    "                        loss = loss / self.gradient_accumulate_every\n",
    "                        total_loss += loss.item()\n",
    "\n",
    "                    self.accelerator.backward(loss)\n",
    "\n",
    "                pbar.set_description(f'loss: {total_loss:.4f}')\n",
    "\n",
    "                accelerator.wait_for_everyone()\n",
    "                accelerator.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
    "\n",
    "                self.opt.step()\n",
    "                self.opt.zero_grad()\n",
    "\n",
    "                accelerator.wait_for_everyone()\n",
    "\n",
    "                self.step += 1\n",
    "                if accelerator.is_main_process:\n",
    "                    self.ema.update()\n",
    "\n",
    "                    if self.step != 0 and divisible_by(self.step, self.save_and_sample_every):\n",
    "                        self.ema.ema_model.eval()\n",
    "\n",
    "                        with torch.inference_mode():\n",
    "                            milestone = self.step // self.save_and_sample_every\n",
    "                            batches = num_to_groups(self.num_samples, self.batch_size)\n",
    "                            all_images_list = list(map(lambda n: self.ema.ema_model.sample(batch_size=n), batches))\n",
    "\n",
    "                        all_images = torch.cat(all_images_list, dim = 0)\n",
    "\n",
    "                        utils.save_image(all_images, str(self.results_folder / f'sample-{milestone}.png'), nrow = int(math.sqrt(self.num_samples)))\n",
    "\n",
    "                        # whether to calculate fid\n",
    "\n",
    "                        if self.calculate_fid:\n",
    "                            fid_score = self.fid_scorer.fid_score()\n",
    "                            accelerator.print(f'fid_score: {fid_score}')\n",
    "                        if self.save_best_and_latest_only:\n",
    "                            if self.best_fid > fid_score:\n",
    "                                self.best_fid = fid_score\n",
    "                                self.save(\"best\")\n",
    "                            self.save(\"latest\")\n",
    "                        else:\n",
    "                            self.save(milestone)\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "        accelerator.print('training complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b964cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 활용\n",
    "trainer = Trainer(\n",
    "    diffusion,\n",
    "    'path/to/your/images',\n",
    "    train_batch_size = 32,\n",
    "    train_lr = 8e-5,\n",
    "    train_num_steps = 700000,         # total training steps\n",
    "    gradient_accumulate_every = 2,    # gradient accumulation steps\n",
    "    ema_decay = 0.995,                # exponential moving average decay\n",
    "    amp = True,                       # turn on mixed precision\n",
    "    calculate_fid = True              # whether to calculate fid during training\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad1a27b",
   "metadata": {},
   "source": [
    "# 간단한 코드 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd24603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from denoising_diffusion_pytorch import Unet, GaussianDiffusion, Trainer\n",
    "\n",
    "model = Unet(\n",
    "    dim = 64,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    flash_attn = True\n",
    ")\n",
    "\n",
    "diffusion = GaussianDiffusion(\n",
    "    model,\n",
    "    image_size = 128,\n",
    "    timesteps = 1000,           # number of steps\n",
    "    sampling_timesteps = 250    # number of sampling timesteps (using ddim for faster inference [see citation for ddim paper])\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    diffusion,\n",
    "    'path/to/your/images',\n",
    "    train_batch_size = 32,\n",
    "    train_lr = 8e-5,\n",
    "    train_num_steps = 700000,         # total training steps\n",
    "    gradient_accumulate_every = 2,    # gradient accumulation steps\n",
    "    ema_decay = 0.995,                # exponential moving average decay\n",
    "    amp = True,                       # turn on mixed precision\n",
    "    calculate_fid = True              # whether to calculate fid during training\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aimv",
   "language": "python",
   "name": "aimv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
